{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ae869147",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ETL Done ✔\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import logging\n",
    "import sqlite3\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# ===========================================================\n",
    "#                 CONFIGURATION\n",
    "# ===========================================================\n",
    "BASE_PATH = r\"E:\\work\\DEPI\\graduation promax\"\n",
    "RAW_DB = fr\"{BASE_PATH}\\data\\raw\\ivf_database_updated.db\"\n",
    "STAR_DB = fr\"{BASE_PATH}\\data\\warehouse_final\\ivf_star_schema.db\"\n",
    "SCHEMA_SQL = fr\"{BASE_PATH}\\src\\ETL\\create_star_schema.sql\"\n",
    "LOG_FILE = fr\"{BASE_PATH}\\src\\ETL\\logs\\etl_log_ivf.txt\"\n",
    "\n",
    "os.makedirs(os.path.dirname(LOG_FILE), exist_ok=True)\n",
    "os.makedirs(os.path.dirname(STAR_DB), exist_ok=True)\n",
    "\n",
    "logging.basicConfig(\n",
    "    filename=LOG_FILE,\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s | %(levelname)s | %(message)s\"\n",
    ")\n",
    "\n",
    "# ===========================================================\n",
    "#                SCHEMA SQL (FULL REFRESH)\n",
    "# ===========================================================\n",
    "def run_schema_sql():\n",
    "    conn = sqlite3.connect(STAR_DB)\n",
    "    with open(SCHEMA_SQL, \"r\", encoding=\"utf-8\") as f:\n",
    "        conn.executescript(f.read())\n",
    "    conn.commit()\n",
    "    conn.close()\n",
    "    logging.info(\"Schema (fresh) created successfully.\")\n",
    "\n",
    "\n",
    "# ===========================================================\n",
    "#                   RAW LOADING\n",
    "# ===========================================================\n",
    "def load_raw_df():\n",
    "    conn = sqlite3.connect(RAW_DB)\n",
    "    df = pd.read_sql(\"SELECT * FROM ivf_patients\", conn)\n",
    "    conn.close()\n",
    "    logging.info(f\"Loaded {len(df)} raw rows.\")\n",
    "    return df\n",
    "\n",
    "\n",
    "# ===========================================================\n",
    "#                   CLEAN DATA\n",
    "# ===========================================================\n",
    "def clean_data(df):\n",
    "    df.columns = df.columns.str.strip().str.lower().str.replace(\" \", \"_\")\n",
    "    df = df.drop_duplicates()\n",
    "    return df\n",
    "\n",
    "\n",
    "# ===========================================================\n",
    "#    CHECK REQUIRED COLUMNS (ONLY 3)\n",
    "# ===========================================================\n",
    "def check_required(df):\n",
    "    required = [\"case_id\", \"female_id\", \"male_id\"]\n",
    "    missing = [c for c in required if c not in df.columns]\n",
    "    if missing:\n",
    "        raise ValueError(f\"Missing required columns: {missing}\")\n",
    "\n",
    "\n",
    "# ===========================================================\n",
    "#   SET NON-CRITICAL IDs TO NULL\n",
    "# ===========================================================\n",
    "def handle_ids(df):\n",
    "    id_cols = [\"protocol_id\", \"outcome_id\", \"embryo_id\"]\n",
    "    for c in id_cols:\n",
    "        if c not in df.columns:\n",
    "            df[c] = None\n",
    "\n",
    "    if \"fresh_et_stage\" not in df.columns:\n",
    "        df[\"fresh_et_stage\"] = None\n",
    "    if \"grading\" not in df.columns:\n",
    "        df[\"grading\"] = None\n",
    "\n",
    "    df[\"transfer_time_id\"] = pd.to_datetime(df.get(\"et_date\", None), errors=\"coerce\") \\\n",
    "                                .dt.strftime(\"%Y-%m-%d\")\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "# ===========================================================\n",
    "#     MAP DOCTOR NAME → AUTO INCREMENT doctor_id\n",
    "# ===========================================================\n",
    "def map_doctor_ids(df, conn):\n",
    "    if \"doctor_name\" not in df.columns:\n",
    "        df[\"doctor_name\"] = \"Unknown\"\n",
    "\n",
    "    # Read existing doctors\n",
    "    existing = pd.read_sql(\"SELECT doctor_id, doctor_name FROM dim_doctor;\", conn)\n",
    "    name_to_id = dict(zip(existing[\"doctor_name\"], existing[\"doctor_id\"]))\n",
    "\n",
    "    unique_names = df[\"doctor_name\"].dropna().unique()\n",
    "    new_doctors = [name for name in unique_names if name not in name_to_id]\n",
    "\n",
    "    for name in new_doctors:\n",
    "        cur = conn.execute(\n",
    "            \"INSERT INTO dim_doctor (doctor_name) VALUES (?)\",\n",
    "            (name,)\n",
    "        )\n",
    "        name_to_id[name] = cur.lastrowid\n",
    "\n",
    "    df[\"doctor_id\"] = df[\"doctor_name\"].map(name_to_id)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "# ===========================================================\n",
    "#   SAFE INSERT → NO DUPLICATION\n",
    "# ===========================================================\n",
    "def insert_or_ignore(table, df_subset, conn):\n",
    "    cols = df_subset.columns.tolist()\n",
    "    placeholders = \",\".join(\"?\" * len(cols))\n",
    "    sql = f\"INSERT OR IGNORE INTO {table} ({','.join(cols)}) VALUES ({placeholders})\"\n",
    "    conn.executemany(sql, df_subset.values.tolist())\n",
    "    conn.commit()\n",
    "\n",
    "\n",
    "# ===========================================================\n",
    "#       DIMENSIONS LOADING\n",
    "# ===========================================================\n",
    "def load_dimensions(df, conn, refresh=True):\n",
    "    dim_tables = {\n",
    "        \"dim_female\":  [\"female_id\",\"female_age\",\"female_bmi\",\"amh_level\",\"fsh_level\",\"afc\"],\n",
    "        \"dim_male\":    [\"male_id\",\"male_age\",\"male_factor\",\"semen_count_mill_per_ml\",\n",
    "                        \"motility_percent\",\"morphology_percent\"],\n",
    "        \"dim_protocol\":[\"protocol_id\",\"protocol_type\",\"stimulation_days\",\n",
    "                        \"total_fsh_dose\",\"trigger_type\",\"recommended_protocol\"],\n",
    "        \"dim_outcome\": [\"outcome_id\",\"risk_level\",\"response_type\",\n",
    "                        \"suggested_waiting_period_days\",\"failure_reason\"],\n",
    "        \"dim_embryo\":  [\"embryo_id\",\"fresh_et_stage\",\"grading\",\"class_a_rate\"]\n",
    "    }\n",
    "\n",
    "    for table, cols in dim_tables.items():\n",
    "        subset = df[cols].drop_duplicates()\n",
    "\n",
    "        if refresh:\n",
    "            subset.to_sql(table, conn, if_exists=\"replace\", index=False)\n",
    "        else:\n",
    "            insert_or_ignore(table, subset, conn)\n",
    "\n",
    "        logging.info(f\"{table}: {len(subset)} processed.\")\n",
    "\n",
    "\n",
    "# ===========================================================\n",
    "#       DIM_TIME\n",
    "# ===========================================================\n",
    "def build_dim_time(df, conn):\n",
    "    tmp = pd.to_datetime(df.get(\"et_date\", None), errors=\"coerce\").dropna().drop_duplicates()\n",
    "\n",
    "    time_dim = pd.DataFrame({\n",
    "        \"full_date\": tmp.dt.strftime(\"%Y-%m-%d\"),\n",
    "        \"day\": tmp.dt.day,\n",
    "        \"month\": tmp.dt.month,\n",
    "        \"month_name\": tmp.dt.month_name(),\n",
    "        \"quarter\": tmp.dt.quarter,\n",
    "        \"year\": tmp.dt.year,\n",
    "        \"week\": tmp.dt.isocalendar().week.astype(int)\n",
    "    })\n",
    "\n",
    "    for _, row in time_dim.iterrows():\n",
    "        conn.execute(\"\"\"\n",
    "            INSERT OR IGNORE INTO dim_time\n",
    "            (full_date, day, month, month_name, quarter, year, week)\n",
    "            VALUES (?, ?, ?, ?, ?, ?, ?)\n",
    "        \"\"\", tuple(row))\n",
    "    conn.commit()\n",
    "\n",
    "\n",
    "# ===========================================================\n",
    "#              FACT TABLES\n",
    "# ===========================================================\n",
    "def load_fact_tables(df, conn):\n",
    "    time_df = pd.read_sql(\"SELECT time_id, full_date FROM dim_time;\", conn)\n",
    "    date_to_id = dict(zip(time_df[\"full_date\"], time_df[\"time_id\"]))\n",
    "\n",
    "    # Fill missing fact numeric columns with 0\n",
    "    fact_needed = [\n",
    "        \"e2_on_trigger\",\"endometrium_thickness\",\"follicles_18mm\",\n",
    "        \"gv_count\",\"injected_m2\",\"fertilized_oocytes\",\n",
    "        \"cleavage_d3\",\"blastocyst_d5\",\"good_embryos\"\n",
    "    ]\n",
    "    for col in fact_needed:\n",
    "        if col not in df.columns:\n",
    "            df[col] = 0\n",
    "\n",
    "    df[\"cycle_start_time_id\"] = df[\"transfer_time_id\"].map(date_to_id)\n",
    "\n",
    "    fact_cycle = df.drop_duplicates(subset=[\"case_id\"])[[\n",
    "        \"case_id\",\"female_id\",\"male_id\",\"protocol_id\",\"doctor_id\",\"outcome_id\",\n",
    "        \"cycle_start_time_id\",\"e2_on_trigger\",\"endometrium_thickness\",\n",
    "        \"follicles_18mm\",\"retrieved_oocytes\",\"m2_count\",\"gv_count\",\n",
    "        \"injected_m2\",\"fertilized_oocytes\",\"fertilization_rate\",\n",
    "        \"cleavage_d3\",\"blastocyst_d5\",\"good_embryos\"\n",
    "    ]]\n",
    "    insert_or_ignore(\"fact_ivf_cycle\", fact_cycle, conn)\n",
    "\n",
    "    # ---------------- FACT TRANSFER ----------------\n",
    "    if all(col in df.columns for col in [\n",
    "        \"case_id\",\"transfer_time_id\",\"doctor_id\",\"embryos_transferred\"\n",
    "    ]):\n",
    "        tmp = df.drop_duplicates(subset=[\"case_id\"]).copy()\n",
    "        tmp[\"transfer_time_fk\"] = tmp[\"transfer_time_id\"].map(date_to_id)\n",
    "        fact_transfer = tmp[[\n",
    "            \"case_id\",\"transfer_time_fk\",\"doctor_id\",\n",
    "            \"embryos_transferred\",\"pregnancy_test_result\",\n",
    "            \"clinical_pregnancy\",\"live_birth\",\n",
    "            \"outcome_id\",\"success_probability_score\"\n",
    "        ]]\n",
    "        insert_or_ignore(\"fact_transfer\", fact_transfer, conn)\n",
    "\n",
    "    # ---------------- FACT TRANSFER EMBRYO ----------------\n",
    "    try:\n",
    "        existing_transfer = pd.read_sql(\"SELECT transfer_sk, case_id FROM fact_transfer;\", conn)\n",
    "        if not existing_transfer.empty and \"embryo_id\" in df.columns:\n",
    "            df_merge = df.merge(existing_transfer, on=\"case_id\", how=\"inner\")\n",
    "            fact_embryo = df_merge[[\"transfer_sk\",\"embryo_id\"]].drop_duplicates()\n",
    "            insert_or_ignore(\"fact_transfer_embryo\", fact_embryo, conn)\n",
    "    except:\n",
    "        logging.warning(\"fact_transfer_embryo skipped.\")\n",
    "\n",
    "\n",
    "# ===========================================================\n",
    "#                    MAIN ETL\n",
    "# ===========================================================\n",
    "def run_full_etl(refresh=True):\n",
    "    logging.info(\"===== ETL STARTED =====\")\n",
    "\n",
    "    if refresh:\n",
    "        run_schema_sql()\n",
    "\n",
    "    df = load_raw_df()\n",
    "    df = clean_data(df)\n",
    "\n",
    "    # Required columns check\n",
    "    check_required(df)\n",
    "\n",
    "    # Normalize MII/M2 naming\n",
    "    rename_map = {\"mii_count\": \"m2_count\", \"injected_mii\": \"injected_m2\"}\n",
    "    df = df.rename(columns={k: v for k, v in rename_map.items() if k in df.columns})\n",
    "\n",
    "    df = handle_ids(df)\n",
    "\n",
    "    conn = sqlite3.connect(STAR_DB)\n",
    "\n",
    "    # Doctor mapping\n",
    "    df = map_doctor_ids(df, conn)\n",
    "\n",
    "    # Dimensions\n",
    "    load_dimensions(df, conn, refresh=refresh)\n",
    "    build_dim_time(df, conn)\n",
    "    load_fact_tables(df, conn)\n",
    "\n",
    "    conn.close()\n",
    "\n",
    "    logging.info(\"ETL COMPLETED SUCCESSFULLY.\")\n",
    "    print(\"ETL Done ✔\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run_full_etl(refresh=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6d404684",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- ALL TABLES IN DB ---\n",
      "                    name\n",
      "0             dim_doctor\n",
      "1        sqlite_sequence\n",
      "2               dim_time\n",
      "3         fact_ivf_cycle\n",
      "4          fact_transfer\n",
      "5   fact_transfer_embryo\n",
      "6             dim_female\n",
      "7               dim_male\n",
      "8           dim_protocol\n",
      "9            dim_outcome\n",
      "10            dim_embryo\n",
      "\n",
      "--- ROW COUNTS ---\n",
      "dim_doctor                → 6 rows\n",
      "sqlite_sequence           → 4 rows\n",
      "dim_time                  → 3376 rows\n",
      "fact_ivf_cycle            → 10000 rows\n",
      "fact_transfer             → 30000 rows\n",
      "fact_transfer_embryo      → 60000 rows\n",
      "dim_female                → 30000 rows\n",
      "dim_male                  → 30000 rows\n",
      "dim_protocol              → 29767 rows\n",
      "dim_outcome               → 21750 rows\n",
      "dim_embryo                → 20028 rows\n",
      "\n",
      "--- SAMPLE DATA (LIMIT 3) ---\n",
      "\n",
      "TABLE: dim_doctor\n",
      "   doctor_id doctor_name                              doctor_recommendation\n",
      "0          1     Unknown  Increase gonadotropin dose in next cycle for b...\n",
      "1          2     Unknown  Proceed with embryo freezing for future transfer.\n",
      "2          3     Unknown  Monitor progesterone closely during luteal phase.\n",
      "\n",
      "TABLE: sqlite_sequence\n",
      "             name    seq\n",
      "0      dim_doctor      6\n",
      "1        dim_time  10128\n",
      "2  fact_ivf_cycle  30000\n",
      "\n",
      "TABLE: dim_time\n",
      "   time_id   full_date  day  month month_name  quarter  year  week\n",
      "0        1  2022-03-26   26      3      March        1  2022    12\n",
      "1        2  2016-03-30   30      3      March        1  2016    13\n",
      "2        3  2018-09-06    6      9  September        3  2018    36\n",
      "\n",
      "TABLE: fact_ivf_cycle\n",
      "   cycle_sk     case_id                             female_id  \\\n",
      "0         1  CASE100000  427ba873-37d2-4dd4-9e2f-ee76fd8836fc   \n",
      "1         2  CASE100001  279bd41e-7f8c-4531-8391-19cda6887977   \n",
      "2         3  CASE100002  935afeb1-0f78-4af5-8ae7-a44627e5930e   \n",
      "\n",
      "                                male_id    protocol_id  doctor_id  \\\n",
      "0  81d73ebe-eb4e-4a73-a054-fa643c2901a7  prot_d0c568aa          1   \n",
      "1  8b70da19-1cd9-41a3-b201-87e7c77719c1  prot_f0d19853          2   \n",
      "2  17eb1362-9088-4c4f-8fb5-105b14c27a4e  prot_26c2b0d9          3   \n",
      "\n",
      "     outcome_id  cycle_start_time_id  e2_on_trigger  endometrium_thickness  \\\n",
      "0  out_421fda04                    1         2970.1                    6.8   \n",
      "1  out_563fda97                    2         1284.1                    6.5   \n",
      "2  out_856f2eb2                    3         2718.6                    5.0   \n",
      "\n",
      "   follicles_18mm  retrieved_oocytes  m2_count  gv_count  injected_m2  \\\n",
      "0              22                  6         5         0            5   \n",
      "1              18                  4         4         0            4   \n",
      "2              18                  4         3         0            3   \n",
      "\n",
      "   fertilized_oocytes  fertilization_rate  cleavage_d3  blastocyst_d5  \\\n",
      "0                   2                 0.4            1              1   \n",
      "1                   2                 0.5            1              0   \n",
      "2                   0                 0.0            0              0   \n",
      "\n",
      "   good_embryos  \n",
      "0             1  \n",
      "1             0  \n",
      "2             0  \n",
      "\n",
      "TABLE: fact_transfer\n",
      "   transfer_sk     case_id  transfer_time_fk  doctor_id  embryos_transferred  \\\n",
      "0            1  CASE100000                 1          1                    2   \n",
      "1            2  CASE100001                 2          2                    1   \n",
      "2            3  CASE100002                 3          3                    1   \n",
      "\n",
      "  pregnancy_test_result clinical_pregnancy live_birth    outcome_id  \\\n",
      "0              Negative                 No         No  out_421fda04   \n",
      "1              Negative                 No         No  out_563fda97   \n",
      "2              Negative                 No         No  out_856f2eb2   \n",
      "\n",
      "   success_probability_score  \n",
      "0                      0.402  \n",
      "1                      0.343  \n",
      "2                      0.324  \n",
      "\n",
      "TABLE: fact_transfer_embryo\n",
      "   transfer_sk     embryo_id\n",
      "0            1  emb_398d64b1\n",
      "1            2  emb_4a0952ad\n",
      "2            3  emb_ace5732e\n",
      "\n",
      "TABLE: dim_female\n",
      "                              female_id  female_age  female_bmi  amh_level  \\\n",
      "0  427ba873-37d2-4dd4-9e2f-ee76fd8836fc          35        23.3       1.65   \n",
      "1  279bd41e-7f8c-4531-8391-19cda6887977          32        24.8       0.71   \n",
      "2  935afeb1-0f78-4af5-8ae7-a44627e5930e          36        23.6       0.76   \n",
      "\n",
      "   fsh_level  afc  \n",
      "0       4.66    8  \n",
      "1       3.70    9  \n",
      "2      13.85   13  \n",
      "\n",
      "TABLE: dim_male\n",
      "                                male_id  male_age male_factor  \\\n",
      "0  81d73ebe-eb4e-4a73-a054-fa643c2901a7        41      Normal   \n",
      "1  8b70da19-1cd9-41a3-b201-87e7c77719c1        38      Normal   \n",
      "2  17eb1362-9088-4c4f-8fb5-105b14c27a4e        36         OAT   \n",
      "\n",
      "   semen_count_mill_per_ml  motility_percent  morphology_percent  \n",
      "0                     43.0              20.5                 8.2  \n",
      "1                     58.7              22.1                 2.5  \n",
      "2                     88.2              41.6                 5.4  \n",
      "\n",
      "TABLE: dim_protocol\n",
      "     protocol_id protocol_type  stimulation_days  total_fsh_dose trigger_type  \\\n",
      "0  prot_d0c568aa    Antagonist                 9            3598          hCG   \n",
      "1  prot_f0d19853          Mild                14            4051          hCG   \n",
      "2  prot_26c2b0d9          Long                14            4833         Dual   \n",
      "\n",
      "  recommended_protocol  \n",
      "0           Antagonist  \n",
      "1              Agonist  \n",
      "2              Agonist  \n",
      "\n",
      "TABLE: dim_outcome\n",
      "     outcome_id risk_level response_type  suggested_waiting_period_days  \\\n",
      "0  out_421fda04   Moderate        Normal                            138   \n",
      "1  out_563fda97       High          Poor                             65   \n",
      "2  out_856f2eb2       High          Poor                            170   \n",
      "\n",
      "         failure_reason  \n",
      "0               Unknown  \n",
      "1               Unknown  \n",
      "2  Implantation Failure  \n",
      "\n",
      "TABLE: dim_embryo\n",
      "      embryo_id fresh_et_stage grading  class_a_rate\n",
      "0  emb_398d64b1             D3       A           1.0\n",
      "1  emb_4a0952ad             D5       C           0.0\n",
      "2  emb_ace5732e             D5       C           0.0\n"
     ]
    }
   ],
   "source": [
    "import sqlite3\n",
    "import pandas as pd\n",
    "\n",
    "DB = r\"E:\\work\\DEPI\\graduation promax\\data\\warehouse_final\\ivf_star_schema.db\"\n",
    "conn = sqlite3.connect(DB)\n",
    "\n",
    "# 1️⃣  عرض كل الجداول الموجودة فعليًا:\n",
    "print(\"\\n--- ALL TABLES IN DB ---\")\n",
    "tables = pd.read_sql(\"SELECT name FROM sqlite_master WHERE type='table';\", conn)\n",
    "print(tables)\n",
    "\n",
    "# 2️⃣  عرض عدد الصفوف لكل جدول:\n",
    "print(\"\\n--- ROW COUNTS ---\")\n",
    "for t in tables['name']:\n",
    "    count = pd.read_sql(f\"SELECT COUNT(*) as rows FROM {t};\", conn)\n",
    "    print(f\"{t:<25} → {count['rows'][0]} rows\")\n",
    "\n",
    "# 3️⃣  عرض أول 3 صفوف من كل جدول (لو فيه بيانات):\n",
    "print(\"\\n--- SAMPLE DATA (LIMIT 3) ---\")\n",
    "for t in tables['name']:\n",
    "    try:\n",
    "        sample = pd.read_sql(f\"SELECT * FROM {t} LIMIT 3;\", conn)\n",
    "        print(f\"\\nTABLE: {t}\")\n",
    "        print(sample)\n",
    "    except:\n",
    "        print(f\"\\nTABLE: {t} → Error or no rows\")\n",
    "\n",
    "conn.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c987f87e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   cid        name     type  notnull dflt_value  pk\n",
      "0    0     time_id  INTEGER        0       None   1\n",
      "1    1   full_date     TEXT        0       None   0\n",
      "2    2         day  INTEGER        0       None   0\n",
      "3    3       month  INTEGER        0       None   0\n",
      "4    4  month_name     TEXT        0       None   0\n",
      "5    5     quarter  INTEGER        0       None   0\n",
      "6    6        year  INTEGER        0       None   0\n",
      "7    7        week  INTEGER        0       None   0\n",
      "   time_id   full_date  day  month month_name  quarter  year  week\n",
      "0        1  2022-03-26   26      3      March        1  2022    12\n",
      "1        2  2016-03-30   30      3      March        1  2016    13\n",
      "2        3  2018-09-06    6      9  September        3  2018    36\n",
      "3        4  2018-03-05    5      3      March        1  2018    10\n",
      "4        5  2023-03-07    7      3      March        1  2023    10\n"
     ]
    }
   ],
   "source": [
    "import sqlite3, pandas as pd\n",
    "conn = sqlite3.connect(STAR_DB)\n",
    "\n",
    "df = pd.read_sql(\"PRAGMA table_info(dim_time);\", conn)\n",
    "print(df)\n",
    "\n",
    "df = pd.read_sql(\"SELECT * FROM dim_time LIMIT 5;\", conn)\n",
    "print(df)\n",
    "\n",
    "\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "296afc38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Total Female Patients ---\n",
      "   COUNT(*)\n",
      "0     10000\n",
      "\n",
      "--- Total Male Patients ---\n",
      "   COUNT(*)\n",
      "0     10000\n",
      "\n",
      "--- Protocol Distribution ---\n",
      "  protocol_type  COUNT(*)\n",
      "0    Antagonist      5307\n",
      "1          Long      2519\n",
      "2          Mild       783\n",
      "3         Short      1158\n",
      "\n",
      "--- Success Outcome Counts ---\n",
      "  outcome_id  COUNT(*)\n",
      "0       None     10000\n",
      "\n",
      "--- Sample Dates (dim_time) ---\n",
      "   time_id   full_date  day  month month_name  quarter  year  week\n",
      "0        1  2022-03-26   26      3      March        1  2022    12\n",
      "1        2  2016-03-30   30      3      March        1  2016    13\n",
      "2        3  2018-09-06    6      9  September        3  2018    36\n",
      "3        4  2018-03-05    5      3      March        1  2018    10\n",
      "4        5  2023-03-07    7      3      March        1  2023    10\n",
      "\n",
      "Connection closed.\n"
     ]
    }
   ],
   "source": [
    "import sqlite3\n",
    "import pandas as pd\n",
    "\n",
    "DB_PATH = r\"E:\\work\\DEPI\\graduation promax\\data\\warehouse_final\\ivf_star_schema.db\"\n",
    "\n",
    "# Connect\n",
    "conn = sqlite3.connect(DB_PATH)\n",
    "\n",
    "queries = {\n",
    "    \"Total Female Patients\": \"SELECT COUNT(*) FROM dim_female;\",\n",
    "    \"Total Male Patients\": \"SELECT COUNT(*) FROM dim_male;\",\n",
    "    \"Protocol Distribution\": \"\"\"\n",
    "        SELECT protocol_type, COUNT(*) \n",
    "        FROM dim_protocol\n",
    "        GROUP BY protocol_type;\n",
    "    \"\"\",\n",
    "    \"Success Outcome Counts\": \"\"\"\n",
    "        SELECT outcome_id, COUNT(*) \n",
    "        FROM fact_transfer\n",
    "        GROUP BY outcome_id;\n",
    "    \"\"\",\n",
    "    \"Sample Dates (dim_time)\": \"\"\"\n",
    "        SELECT * FROM dim_time LIMIT 5;\n",
    "    \"\"\"\n",
    "}\n",
    "\n",
    "for title, q in queries.items():\n",
    "    print(f\"\\n--- {title} ---\")\n",
    "    try:\n",
    "        df = pd.read_sql(q, conn)\n",
    "        print(df)\n",
    "    except Exception as e:\n",
    "        print(\"Error:\", e)\n",
    "\n",
    "conn.close()\n",
    "print(\"\\nConnection closed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "82f23ec8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['case_id', 'et_date', 'female_age', 'female_bmi', 'amh_level',\n",
      "       'fsh_level', 'afc', 'male_age', 'male_factor',\n",
      "       'semen_count_mill_per_ml', 'motility_percent', 'morphology_percent',\n",
      "       'protocol_type', 'stimulation_days', 'total_fsh_dose', 'trigger_type',\n",
      "       'e2_on_trigger', 'endometrium_thickness', 'follicles_18mm',\n",
      "       'retrieved_oocytes', 'mii_count', 'mi_count', 'gv_count',\n",
      "       'injected_mii', 'fertilized_oocytes', 'fertilization_rate',\n",
      "       'cleavage_d3', 'blastocyst_d5', 'good_embryos', 'class_a_rate',\n",
      "       'fresh_et_stage', 'embryos_transferred', 'grading',\n",
      "       'pregnancy_test_result', 'clinical_pregnancy', 'live_birth',\n",
      "       'success_probability_score', 'response_type', 'risk_level',\n",
      "       'recommended_protocol', 'suggested_waiting_period_days',\n",
      "       'failure_reason', 'doctor_recommendation', 'female_id', 'male_id',\n",
      "       'transfer_time_id', 'protocol_id', 'doctor_id', 'outcome_id',\n",
      "       'embryo_id'],\n",
      "      dtype='object')\n",
      "      case_id transfer_time_id  embryos_transferred\n",
      "0  CASE100000       2022-03-26                    2\n",
      "1  CASE100001       2016-03-30                    1\n",
      "2  CASE100002       2018-09-06                    1\n",
      "3  CASE100003       2018-03-05                    1\n",
      "4  CASE100004       2023-03-07                    2\n",
      "embryos_transferred\n",
      "1    5516\n",
      "2    4096\n",
      "3     388\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "df = load_raw_df()\n",
    "df = clean_data(df)\n",
    "df = apply_placeholder_and_ids(df)\n",
    "print(df.columns)   # عشان نتأكد الأعمدة موجودة\n",
    "\n",
    "print(df[[\"case_id\", \"transfer_time_id\", \"embryos_transferred\"]].head())  # نشوف عينات\n",
    "\n",
    "print(df[\"embryos_transferred\"].value_counts())  # نعرف لو كلها صفر أو فاضية\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d15c042b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✔ Test DB created successfully: ivf_patients_test.db\n"
     ]
    }
   ],
   "source": [
    "import sqlite3\n",
    "import pandas as pd\n",
    "\n",
    "# إنشاء DB جديدة\n",
    "conn = sqlite3.connect(r\"E:\\work\\DEPI\\graduation promax\\data\\raw\\ivf_patients_test.db\")\n",
    "\n",
    "# الداتا كاملة بنفس الشكل المطلوب للـ ETL\n",
    "data = [\n",
    "    # صف مكرر (للاختبار)\n",
    "    [\"CASE_TEST_001\", \"2023-03-07\", 32, 24.8, 1.2, 5.0, 10, 38, \"Normal\", 55.0, 20.0, 5.0, 4, 4, 2, 0.5, 1, 1.0, \"D5\", 2, \"B\", \"Negative\", 0, 0, 0.76, \"Poor\", \"High\", \"Antagonist\", 90, \"Unknown\", \"Good response\"],\n",
    "    [\"CASE_TEST_001\", \"2023-03-07\", 32, 24.8, 1.2, 5.0, 10, 38, \"Normal\", 55.0, 20.0, 5.0, 4, 4, 2, 0.5, 1, 1.0, \"D5\", 2, \"B\", \"Negative\", 0, 0, 0.76, \"Poor\", \"High\", \"Antagonist\", 90, \"Unknown\", \"Good response\"],\n",
    "\n",
    "    # صف جديد\n",
    "    [\"CASE_TEST_002\", \"2022-11-15\", 30, 22.5, 0.9, 4.3, 12, 41, \"OAT\", 40.0, 18.0, 6.0, 5, 5, 3, 0.6, 2, 0.8, \"D3\", 1, \"C\", \"Positive\", 1, 0, 0.88, \"Normal\", \"Medium\", \"Mild\", 45, \"Implantation Failure\", \"Monitor closely\"],\n",
    "\n",
    "    # صف جديد (نصف البيانات ناقص → اختبار null handling)\n",
    "    [\"CASE_TEST_003\", None, 29, None, None, 3.1, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, 0.00, None, None, None, None, None, None]\n",
    "]\n",
    "\n",
    "columns = [\n",
    "    \"case_id\",\"et_date\",\"female_age\",\"female_bmi\",\"amh_level\",\"fsh_level\",\"afc\",\n",
    "    \"male_age\",\"male_factor\",\"semen_count_mill_per_ml\",\"motility_percent\",\"morphology_percent\",\n",
    "    \"retrieved_oocytes\",\"mii_count\",\"num_embryos_generated\",\"fertilization_rate\",\n",
    "    \"good_embryos\",\"class_a_rate\",\"fresh_et_stage\",\"embryos_transferred\",\"grading\",\n",
    "    \"pregnancy_test_result\",\"clinical_pregnancy\",\"live_birth\",\"success_probability_score\",\n",
    "    \"response_type\",\"risk_level\",\"recommended_protocol\",\"suggested_waiting_period_days\",\n",
    "    \"failure_reason\",\"doctor_recommendation\"\n",
    "]\n",
    "\n",
    "df = pd.DataFrame(data, columns=columns)\n",
    "df.to_sql(\"ivf_patients\", conn, if_exists=\"replace\", index=False)\n",
    "\n",
    "conn.close()\n",
    "print(\"✔ Test DB created successfully: ivf_patients_test.db\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "972cb598",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>case_id</th>\n",
       "      <th>et_date</th>\n",
       "      <th>female_age</th>\n",
       "      <th>female_bmi</th>\n",
       "      <th>amh_level</th>\n",
       "      <th>fsh_level</th>\n",
       "      <th>afc</th>\n",
       "      <th>male_age</th>\n",
       "      <th>male_factor</th>\n",
       "      <th>semen_count_mill_per_ml</th>\n",
       "      <th>...</th>\n",
       "      <th>pregnancy_test_result</th>\n",
       "      <th>clinical_pregnancy</th>\n",
       "      <th>live_birth</th>\n",
       "      <th>success_probability_score</th>\n",
       "      <th>response_type</th>\n",
       "      <th>risk_level</th>\n",
       "      <th>recommended_protocol</th>\n",
       "      <th>suggested_waiting_period_days</th>\n",
       "      <th>failure_reason</th>\n",
       "      <th>doctor_recommendation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CASE_TEST_001</td>\n",
       "      <td>2023-03-07</td>\n",
       "      <td>32</td>\n",
       "      <td>24.8</td>\n",
       "      <td>1.2</td>\n",
       "      <td>5.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>Normal</td>\n",
       "      <td>55.0</td>\n",
       "      <td>...</td>\n",
       "      <td>Negative</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.76</td>\n",
       "      <td>Poor</td>\n",
       "      <td>High</td>\n",
       "      <td>Antagonist</td>\n",
       "      <td>90.0</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>Good response</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CASE_TEST_001</td>\n",
       "      <td>2023-03-07</td>\n",
       "      <td>32</td>\n",
       "      <td>24.8</td>\n",
       "      <td>1.2</td>\n",
       "      <td>5.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>Normal</td>\n",
       "      <td>55.0</td>\n",
       "      <td>...</td>\n",
       "      <td>Negative</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.76</td>\n",
       "      <td>Poor</td>\n",
       "      <td>High</td>\n",
       "      <td>Antagonist</td>\n",
       "      <td>90.0</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>Good response</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CASE_TEST_002</td>\n",
       "      <td>2022-11-15</td>\n",
       "      <td>30</td>\n",
       "      <td>22.5</td>\n",
       "      <td>0.9</td>\n",
       "      <td>4.3</td>\n",
       "      <td>12.0</td>\n",
       "      <td>41.0</td>\n",
       "      <td>OAT</td>\n",
       "      <td>40.0</td>\n",
       "      <td>...</td>\n",
       "      <td>Positive</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.88</td>\n",
       "      <td>Normal</td>\n",
       "      <td>Medium</td>\n",
       "      <td>Mild</td>\n",
       "      <td>45.0</td>\n",
       "      <td>Implantation Failure</td>\n",
       "      <td>Monitor closely</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CASE_TEST_003</td>\n",
       "      <td>None</td>\n",
       "      <td>29</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.00</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         case_id     et_date  female_age  female_bmi  amh_level  fsh_level  \\\n",
       "0  CASE_TEST_001  2023-03-07          32        24.8        1.2        5.0   \n",
       "1  CASE_TEST_001  2023-03-07          32        24.8        1.2        5.0   \n",
       "2  CASE_TEST_002  2022-11-15          30        22.5        0.9        4.3   \n",
       "3  CASE_TEST_003        None          29         NaN        NaN        3.1   \n",
       "\n",
       "    afc  male_age male_factor  semen_count_mill_per_ml  ...  \\\n",
       "0  10.0      38.0      Normal                     55.0  ...   \n",
       "1  10.0      38.0      Normal                     55.0  ...   \n",
       "2  12.0      41.0         OAT                     40.0  ...   \n",
       "3   NaN       NaN        None                      NaN  ...   \n",
       "\n",
       "   pregnancy_test_result  clinical_pregnancy  live_birth  \\\n",
       "0               Negative                 0.0         0.0   \n",
       "1               Negative                 0.0         0.0   \n",
       "2               Positive                 1.0         0.0   \n",
       "3                   None                 NaN         NaN   \n",
       "\n",
       "   success_probability_score  response_type  risk_level  recommended_protocol  \\\n",
       "0                       0.76           Poor        High            Antagonist   \n",
       "1                       0.76           Poor        High            Antagonist   \n",
       "2                       0.88         Normal      Medium                  Mild   \n",
       "3                       0.00           None        None                  None   \n",
       "\n",
       "   suggested_waiting_period_days        failure_reason  doctor_recommendation  \n",
       "0                           90.0               Unknown          Good response  \n",
       "1                           90.0               Unknown          Good response  \n",
       "2                           45.0  Implantation Failure        Monitor closely  \n",
       "3                            NaN                  None                   None  \n",
       "\n",
       "[4 rows x 31 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the database and display full table\n",
    "import sqlite3\n",
    "import pandas as pd\n",
    "\n",
    "db_path = r\"E:\\work\\DEPI\\graduation promax\\data\\raw\\ivf_patients_test.db\"\n",
    "conn = sqlite3.connect(db_path)\n",
    "\n",
    "df = pd.read_sql(\"SELECT * FROM ivf_patients\", conn)\n",
    "conn.close()\n",
    "\n",
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "94c30c1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done! Excel saved at: E:\\work\\DEPI\\graduation promax\\data\\raw\\raw_database_dump.xlsx\n"
     ]
    }
   ],
   "source": [
    "import sqlite3\n",
    "import pandas as pd\n",
    "\n",
    "# ================================\n",
    "# 1) PATHS\n",
    "# ================================\n",
    "db_path = r\"E:\\work\\DEPI\\graduation promax\\data\\raw\\ivf_database_updated.db\"\n",
    "excel_output = r\"E:\\work\\DEPI\\graduation promax\\data\\raw\\raw_database_dump.xlsx\"\n",
    "\n",
    "# ================================\n",
    "# 2) CONNECT TO DB\n",
    "# ================================\n",
    "conn = sqlite3.connect(db_path)\n",
    "\n",
    "# ================================\n",
    "# 3) READ TABLE NAMES\n",
    "# ================================\n",
    "tables = pd.read_sql(\n",
    "    \"SELECT name FROM sqlite_master WHERE type='table' AND name NOT LIKE 'sqlite_%'\",\n",
    "    conn\n",
    ")\n",
    "\n",
    "table_list = tables[\"name\"].tolist()\n",
    "\n",
    "# ================================\n",
    "# 4) EXPORT TO EXCEL\n",
    "# ================================\n",
    "with pd.ExcelWriter(excel_output, engine=\"openpyxl\") as writer:\n",
    "    for tbl in table_list:\n",
    "        df = pd.read_sql(f\"SELECT * FROM {tbl}\", conn)\n",
    "        df.to_excel(writer, sheet_name=tbl[:31], index=False)  # Excel sheet name limit\n",
    "\n",
    "# ================================\n",
    "# 5) CLOSE CONNECTION\n",
    "# ================================\n",
    "conn.close()\n",
    "\n",
    "print(\"Done! Excel saved at:\", excel_output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4facd6fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "import pandas as pd\n",
    "\n",
    "# ================================\n",
    "# 1) PATHS\n",
    "# ================================\n",
    "db_path = r\"E:\\work\\DEPI\\graduation promax\\data\\warehouse_final\\ivf_star_schema.db\"\n",
    "excel_output = r\"E:\\work\\DEPI\\graduation promax\\data\\warehouse_final\\database_dump.xlsx\"\n",
    "\n",
    "# ================================\n",
    "# 2) CONNECT TO DB\n",
    "# ================================\n",
    "conn = sqlite3.connect(db_path)\n",
    "\n",
    "# ================================\n",
    "# 3) READ TABLE NAMES\n",
    "# ================================\n",
    "tables = pd.read_sql(\n",
    "    \"SELECT name FROM sqlite_master WHERE type='table' AND name NOT LIKE 'sqlite_%'\",\n",
    "    conn\n",
    ")\n",
    "\n",
    "table_list = tables[\"name\"].tolist()\n",
    "\n",
    "# ================================\n",
    "# 4) EXPORT TO EXCEL\n",
    "# ================================\n",
    "with pd.ExcelWriter(excel_output, engine=\"openpyxl\") as writer:\n",
    "    for tbl in table_list:\n",
    "        df = pd.read_sql(f\"SELECT * FROM {tbl}\", conn)\n",
    "        df.to_excel(writer, sheet_name=tbl[:31], index=False)  # Excel sheet name limit\n",
    "\n",
    "# ================================\n",
    "# 5) CLOSE CONNECTION\n",
    "# ================================\n",
    "conn.close()\n",
    "\n",
    "print(\"Done! Excel saved at:\", excel_output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6fd1d69f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ETL Done ✔\n"
     ]
    }
   ],
   "source": [
    "# etl_sequential_ids.py\n",
    "import os\n",
    "import logging\n",
    "import sqlite3\n",
    "import uuid\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# ----------------------------\n",
    "# CONFIG\n",
    "# ----------------------------\n",
    "BASE_PATH = r\"E:\\work\\DEPI\\graduation promax\"\n",
    "RAW_DB = fr\"{BASE_PATH}\\data\\raw\\ivf_database_updated.db\"\n",
    "STAR_DB = fr\"{BASE_PATH}\\data\\warehouse_final\\ivf_star_schema.db\"\n",
    "SCHEMA_SQL = fr\"{BASE_PATH}\\src\\ETL\\create_star_schema.sql\"  # should contain full create statements\n",
    "LOG_FILE = fr\"{BASE_PATH}\\src\\ETL\\logs\\etl_sequential_ids.txt\"\n",
    "\n",
    "os.makedirs(os.path.dirname(LOG_FILE), exist_ok=True)\n",
    "os.makedirs(os.path.dirname(STAR_DB), exist_ok=True)\n",
    "\n",
    "logging.basicConfig(\n",
    "    filename=LOG_FILE,\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s | %(levelname)s | %(message)s\"\n",
    ")\n",
    "logger = logging.getLogger(\"etl_sequential_ids\")\n",
    "\n",
    "# ----------------------------\n",
    "# DIMENSION DEFINITIONS (natural keys + id col + prefix)\n",
    "# ----------------------------\n",
    "# nat: list of columns that identify a unique row for that dim\n",
    "DIM_DEFS = {\n",
    "    \"dim_female\": {\n",
    "        \"id_col\": \"female_id\",\n",
    "        \"nat\": [\"female_age\", \"female_bmi\", \"amh_level\", \"fsh_level\", \"afc\"],\n",
    "        \"prefix\": \"f\"\n",
    "    },\n",
    "    \"dim_male\": {\n",
    "        \"id_col\": \"male_id\",\n",
    "        \"nat\": [\"male_age\", \"male_factor\", \"semen_count_mill_per_ml\", \"motility_percent\", \"morphology_percent\"],\n",
    "        \"prefix\": \"m\"\n",
    "    },\n",
    "    \"dim_protocol\": {\n",
    "        \"id_col\": \"protocol_id\",\n",
    "        \"nat\": [\"protocol_type\", \"stimulation_days\", \"total_fsh_dose\", \"trigger_type\", \"recommended_protocol\"],\n",
    "        \"prefix\": \"prot\"\n",
    "    },\n",
    "    \"dim_outcome\": {\n",
    "        \"id_col\": \"outcome_id\",\n",
    "        \"nat\": [\"risk_level\", \"response_type\", \"suggested_waiting_period_days\", \"failure_reason\"],\n",
    "        \"prefix\": \"out\"\n",
    "    },\n",
    "    \"dim_embryo\": {\n",
    "        \"id_col\": \"embryo_id\",\n",
    "        \"nat\": [\"fresh_et_stage\", \"grading\", \"class_a_rate\"],\n",
    "        \"prefix\": \"emb\"\n",
    "    },\n",
    "    # dim_doctor handled separately (autoinc integer) using natural key [name, recommendation]\n",
    "}\n",
    "\n",
    "# ----------------------------\n",
    "# HELPERS\n",
    "# ----------------------------\n",
    "def zero_pad(n, width=5):\n",
    "    return str(n).zfill(width)\n",
    "\n",
    "def next_seq_id(conn, table, id_col, prefix):\n",
    "    \"\"\"\n",
    "    Determine next sequential ID in the form prefix_00001\n",
    "    Looks at existing values in table.id_col, extracts numeric suffix and returns next.\n",
    "    If table is empty -> returns prefix_00001\n",
    "    \"\"\"\n",
    "    cur = conn.cursor()\n",
    "    try:\n",
    "        # select maximal numeric suffix by stripping prefix_ and casting\n",
    "        like_pat = f\"{prefix}_%\" \n",
    "        cur.execute(f\"SELECT {id_col} FROM {table} WHERE {id_col} LIKE ? LIMIT 1\", (like_pat,))\n",
    "        # get all matching ids to compute max - simpler and reliable\n",
    "        rows = cur.execute(f\"SELECT {id_col} FROM {table} WHERE {id_col} LIKE ?\", (like_pat,)).fetchall()\n",
    "        if not rows:\n",
    "            return f\"{prefix}_{zero_pad(1)}\"\n",
    "        max_n = 0\n",
    "        for (val,) in rows:\n",
    "            try:\n",
    "                # val like prefix_00012\n",
    "                num = int(val.split(\"_\")[-1])\n",
    "                if num > max_n:\n",
    "                    max_n = num\n",
    "            except Exception:\n",
    "                continue\n",
    "        return f\"{prefix}_{zero_pad(max_n + 1)}\"\n",
    "    finally:\n",
    "        cur.close()\n",
    "\n",
    "def build_nat_key_from_row(row, cols):\n",
    "    # convert NaN -> empty string and strip\n",
    "    vals = []\n",
    "    for c in cols:\n",
    "        v = row.get(c, \"\")\n",
    "        if pd.isna(v):\n",
    "            v = \"\"\n",
    "        vals.append(str(v).strip())\n",
    "    return \"|\".join(vals)\n",
    "\n",
    "def ensure_table_exists(conn, table_sql=None):\n",
    "    \"\"\"If user schema SQL exists, run it earlier. This is a fallback to ensure dim_doctor present.\"\"\"\n",
    "    if table_sql:\n",
    "        conn.executescript(table_sql)\n",
    "        conn.commit()\n",
    "\n",
    "# ----------------------------\n",
    "# CORE: process a dimension using natural keys and sequential ids\n",
    "# ----------------------------\n",
    "def process_dimension(conn, df, dim_name, dim_def, refresh):\n",
    "    id_col = dim_def[\"id_col\"]\n",
    "    nat_cols = dim_def[\"nat\"]\n",
    "    prefix = dim_def[\"prefix\"]\n",
    "\n",
    "    # ensure natural columns exist in incoming df (fill with empty)\n",
    "    for c in nat_cols:\n",
    "        if c not in df.columns:\n",
    "            df[c] = None\n",
    "\n",
    "    # build incoming subset unique by natural key\n",
    "    incoming = df[nat_cols + [id_col]].copy()\n",
    "    incoming = incoming.drop_duplicates(subset=nat_cols).reset_index(drop=True)\n",
    "\n",
    "    # create nat_key column\n",
    "    incoming[\"nat_key\"] = incoming.apply(lambda r: build_nat_key_from_row(r, nat_cols), axis=1)\n",
    "\n",
    "    # fetch existing rows\n",
    "    try:\n",
    "        existing = pd.read_sql(f\"SELECT {id_col}, {', '.join(nat_cols)} FROM {dim_name}\", conn)\n",
    "        if existing.shape[0] > 0:\n",
    "            existing[\"nat_key\"] = existing.apply(lambda r: build_nat_key_from_row(r, nat_cols), axis=1)\n",
    "        else:\n",
    "            existing[\"nat_key\"] = []\n",
    "    except Exception:\n",
    "        # table may not exist yet (schema not created) -> create minimal table with id_col and nat cols\n",
    "        cols_sql = \",\\n\".join([f\"{c} TEXT\" for c in nat_cols])\n",
    "        create_sql = f\"\"\"\n",
    "            CREATE TABLE IF NOT EXISTS {dim_name} (\n",
    "                {id_col} TEXT PRIMARY KEY,\n",
    "                {cols_sql}\n",
    "            );\n",
    "        \"\"\"\n",
    "        conn.execute(create_sql)\n",
    "        conn.commit()\n",
    "        existing = pd.DataFrame(columns=[id_col] + nat_cols + [\"nat_key\"])\n",
    "\n",
    "    nat_to_id = dict(zip(existing[\"nat_key\"].astype(str), existing[id_col].astype(str)))\n",
    "\n",
    "    # Iterate incoming rows: if nat_key exists -> reuse id, else create new sequential id and insert\n",
    "    to_insert = []\n",
    "    assigned_ids = {}\n",
    "    for _, row in incoming.iterrows():\n",
    "        nk = row[\"nat_key\"]\n",
    "        if nk in nat_to_id and nk not in assigned_ids:\n",
    "            assigned_ids[nk] = nat_to_id[nk]\n",
    "            continue\n",
    "        if nk in assigned_ids:\n",
    "            continue\n",
    "        # new natural key -> generate sequential id\n",
    "        new_id = next_seq_id(conn, dim_name, id_col, prefix)\n",
    "        # prepare insert row preserving natural cols values and the id\n",
    "        insert_row = [new_id] + [row[c] if not pd.isna(row[c]) else None for c in nat_cols]\n",
    "        to_insert.append(insert_row)\n",
    "        nat_to_id[nk] = new_id\n",
    "        assigned_ids[nk] = new_id\n",
    "\n",
    "    # Bulk insert new rows (if any)\n",
    "    if to_insert:\n",
    "        placeholders = \",\".join([\"?\"] * (1 + len(nat_cols)))\n",
    "        cols = \", \".join([id_col] + nat_cols)\n",
    "        sql = f\"INSERT OR IGNORE INTO {dim_name} ({cols}) VALUES ({placeholders})\"\n",
    "        cur = conn.cursor()\n",
    "        try:\n",
    "            cur.executemany(sql, to_insert)\n",
    "            conn.commit()\n",
    "            logging.info(f\"{dim_name}: inserted {len(to_insert)} new rows.\")\n",
    "        except Exception as e:\n",
    "            logging.exception(f\"{dim_name}: failed to insert new rows: {e}\")\n",
    "            conn.rollback()\n",
    "            # try row-by-row\n",
    "            for r in to_insert:\n",
    "                try:\n",
    "                    cur.execute(sql, r)\n",
    "                except Exception:\n",
    "                    logging.exception(f\"{dim_name}: single row insert failed: {r}\")\n",
    "            conn.commit()\n",
    "        finally:\n",
    "            cur.close()\n",
    "    else:\n",
    "        logging.info(f\"{dim_name}: no new rows to insert.\")\n",
    "\n",
    "    # Map incoming original df rows to ids by nat_key\n",
    "    # Build mapping nat_key -> id\n",
    "    final_map = nat_to_id  # nat_key -> id\n",
    "\n",
    "    # produce a mapping series for the entire df (not only incoming subset)\n",
    "    df[id_col] = df.apply(lambda r: final_map.get(build_nat_key_from_row(r, nat_cols), None), axis=1)\n",
    "\n",
    "    return df\n",
    "\n",
    "# ----------------------------\n",
    "# Doctor special handling: natural key = name|recommendation, id = autoinc integer\n",
    "# ----------------------------\n",
    "def process_doctors(conn, df, refresh):\n",
    "    if \"doctor_name\" not in df.columns:\n",
    "        df[\"doctor_name\"] = \"Unknown\"\n",
    "    if \"doctor_recommendation\" not in df.columns:\n",
    "        df[\"doctor_recommendation\"] = None\n",
    "\n",
    "    # ensure table exists with UNIQUE(name, recommendation)\n",
    "    conn.execute(\"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS dim_doctor (\n",
    "            doctor_id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "            doctor_name TEXT,\n",
    "            doctor_recommendation TEXT,\n",
    "            UNIQUE (doctor_name, doctor_recommendation)\n",
    "        );\n",
    "    \"\"\")\n",
    "    conn.commit()\n",
    "\n",
    "    df[\"nat_key_doctor\"] = df[\"doctor_name\"].astype(str).str.strip() + \"|\" + df[\"doctor_recommendation\"].astype(str).fillna(\"\").astype(str).str.strip()\n",
    "\n",
    "    # load existing nat keys\n",
    "    existing = pd.read_sql(\"SELECT doctor_id, doctor_name, doctor_recommendation FROM dim_doctor\", conn)\n",
    "    if existing.shape[0] > 0:\n",
    "        existing[\"nat_key_doctor\"] = existing[\"doctor_name\"].astype(str).str.strip() + \"|\" + existing[\"doctor_recommendation\"].astype(str).fillna(\"\").astype(str).str.strip()\n",
    "    else:\n",
    "        existing[\"nat_key_doctor\"] = []\n",
    "\n",
    "    nat_to_id = dict(zip(existing[\"nat_key_doctor\"].astype(str), existing[\"doctor_id\"].astype(int)))\n",
    "\n",
    "    # identify new combinations\n",
    "    new_keys = df.loc[~df[\"nat_key_doctor\"].isin(nat_to_id), [\"doctor_name\", \"doctor_recommendation\", \"nat_key_doctor\"]].drop_duplicates(\"nat_key_doctor\")\n",
    "    cur = conn.cursor()\n",
    "    for _, row in new_keys.iterrows():\n",
    "        try:\n",
    "            cur.execute(\"INSERT OR IGNORE INTO dim_doctor (doctor_name, doctor_recommendation) VALUES (?, ?)\",\n",
    "                        (row[\"doctor_name\"], row[\"doctor_recommendation\"]))\n",
    "            conn.commit()\n",
    "            doc_id = conn.execute(\"SELECT doctor_id FROM dim_doctor WHERE doctor_name = ? AND doctor_recommendation = ?\",\n",
    "                                   (row[\"doctor_name\"], row[\"doctor_recommendation\"])).fetchone()[0]\n",
    "            nat_to_id[row[\"nat_key_doctor\"]] = doc_id\n",
    "        except Exception:\n",
    "            logging.exception(\"process_doctors: insert failed for %s\", row.to_dict())\n",
    "    cur.close()\n",
    "\n",
    "    # assign ids to df\n",
    "    df[\"doctor_id\"] = df[\"nat_key_doctor\"].map(nat_to_id)\n",
    "    df.drop(columns=[\"nat_key_doctor\"], inplace=True)\n",
    "    return df\n",
    "\n",
    "# ----------------------------\n",
    "# dim_time builder (append-only)\n",
    "# ----------------------------\n",
    "def build_dim_time(df, conn):\n",
    "    tmp = pd.to_datetime(df.get(\"et_date\", None), errors=\"coerce\").dropna().drop_duplicates()\n",
    "    if tmp.empty:\n",
    "        return\n",
    "    time_dim = pd.DataFrame({\n",
    "        \"full_date\": tmp.dt.strftime(\"%Y-%m-%d\"),\n",
    "        \"day\": tmp.dt.day,\n",
    "        \"month\": tmp.dt.month,\n",
    "        \"month_name\": tmp.dt.month_name(),\n",
    "        \"quarter\": tmp.dt.quarter,\n",
    "        \"year\": tmp.dt.year,\n",
    "        \"week\": tmp.dt.isocalendar().week.astype(int)\n",
    "    }).drop_duplicates(subset=[\"full_date\"])\n",
    "    cur = conn.cursor()\n",
    "    for _, r in time_dim.iterrows():\n",
    "        try:\n",
    "            cur.execute(\"\"\"INSERT OR IGNORE INTO dim_time\n",
    "                           (full_date, day, month, month_name, quarter, year, week)\n",
    "                           VALUES (?, ?, ?, ?, ?, ?, ?)\"\"\",\n",
    "                        (r.full_date, int(r.day), int(r.month), r.month_name, int(r.quarter), int(r.year), int(r.week)))\n",
    "        except Exception:\n",
    "            logging.exception(\"build_dim_time insert failed for %s\", r.full_date)\n",
    "    conn.commit()\n",
    "    cur.close()\n",
    "\n",
    "# ----------------------------\n",
    "# facts loader (insert or ignore; expects dims already assigned)\n",
    "# ----------------------------\n",
    "def load_fact_tables(df, conn):\n",
    "    # prepare date->time_id map\n",
    "    try:\n",
    "        time_df = pd.read_sql(\"SELECT time_id, full_date FROM dim_time\", conn)\n",
    "    except Exception:\n",
    "        time_df = pd.DataFrame(columns=[\"time_id\", \"full_date\"])\n",
    "    date_to_id = dict(zip(time_df[\"full_date\"], time_df[\"time_id\"]))\n",
    "\n",
    "    # ensure numeric fact columns exist\n",
    "    fact_needed = [\n",
    "        \"e2_on_trigger\",\"endometrium_thickness\",\"follicles_18mm\",\n",
    "        \"retrieved_oocytes\",\"m2_count\",\"gv_count\",\"injected_m2\",\"fertilized_oocytes\",\n",
    "        \"fertilization_rate\",\"cleavage_d3\",\"blastocyst_d5\",\"good_embryos\"\n",
    "    ]\n",
    "    for c in fact_needed:\n",
    "        if c not in df.columns:\n",
    "            df[c] = np.nan\n",
    "\n",
    "    # map time ids\n",
    "    df[\"cycle_start_time_id\"] = df[\"transfer_time_id\"].map(date_to_id)\n",
    "\n",
    "    # build fact_ivf_cycle\n",
    "    fact_cycle_cols = [\n",
    "        \"case_id\",\"female_id\",\"male_id\",\"protocol_id\",\"doctor_id\",\"outcome_id\",\n",
    "        \"cycle_start_time_id\",\"e2_on_trigger\",\"endometrium_thickness\",\"follicles_18mm\",\n",
    "        \"retrieved_oocytes\",\"m2_count\",\"gv_count\",\"injected_m2\",\"fertilized_oocytes\",\n",
    "        \"fertilization_rate\",\"cleavage_d3\",\"blastocyst_d5\",\"good_embryos\"\n",
    "    ]\n",
    "    fc = df.drop_duplicates(subset=[\"case_id\"])[[c for c in fact_cycle_cols if c in df.columns]]\n",
    "    insert_or_ignore(\"fact_ivf_cycle\", fc, conn)\n",
    "\n",
    "    # fact_transfer\n",
    "    if all(c in df.columns for c in [\"case_id\",\"transfer_time_id\",\"doctor_id\",\"embryos_transferred\"]):\n",
    "        tmp = df.drop_duplicates(subset=[\"case_id\"]).copy()\n",
    "        tmp[\"transfer_time_fk\"] = tmp[\"transfer_time_id\"].map(date_to_id)\n",
    "        ft_cols = [\"case_id\",\"transfer_time_fk\",\"doctor_id\",\"embryos_transferred\",\"pregnancy_test_result\",\n",
    "                   \"clinical_pregnancy\",\"live_birth\",\"outcome_id\",\"success_probability_score\"]\n",
    "        ft = tmp[[c for c in ft_cols if c in tmp.columns]]\n",
    "        insert_or_ignore(\"fact_transfer\", ft, conn)\n",
    "\n",
    "    # fact_transfer_embryo\n",
    "    try:\n",
    "        trans = pd.read_sql(\"SELECT transfer_sk, case_id FROM fact_transfer\", conn)\n",
    "        if not trans.empty and \"embryo_id\" in df.columns:\n",
    "            merged = df.merge(trans, on=\"case_id\", how=\"inner\")\n",
    "            te = merged[[\"transfer_sk\", \"embryo_id\"]].drop_duplicates()\n",
    "            insert_or_ignore(\"fact_transfer_embryo\", te, conn)\n",
    "    except Exception:\n",
    "        logging.exception(\"load_fact_tables -> transfer_embryo failed\")\n",
    "\n",
    "# ----------------------------\n",
    "# MAIN ETL\n",
    "# ----------------------------\n",
    "def run_full_etl(refresh=True):\n",
    "    logging.info(\"===== ETL STARTED =====\")\n",
    "    try:\n",
    "        if refresh:\n",
    "            # run schema SQL if provided\n",
    "            if os.path.exists(SCHEMA_SQL):\n",
    "                with open(SCHEMA_SQL, \"r\", encoding=\"utf-8\") as f:\n",
    "                    sql_text = f.read()\n",
    "                conn0 = sqlite3.connect(STAR_DB)\n",
    "                conn0.executescript(sql_text)\n",
    "                conn0.commit()\n",
    "                conn0.close()\n",
    "                logging.info(\"Schema created from SCHEMA_SQL.\")\n",
    "            else:\n",
    "                logging.warning(\"SCHEMA_SQL not found; continuing without executing SQL file.\")\n",
    "\n",
    "        # load raw\n",
    "        conn = sqlite3.connect(RAW_DB)\n",
    "        raw_df = pd.read_sql(\"SELECT * FROM ivf_patients\", conn)\n",
    "        conn.close()\n",
    "        logging.info(\"Loaded raw rows: %d\", len(raw_df))\n",
    "\n",
    "        # clean\n",
    "        raw_df.columns = raw_df.columns.str.strip().str.lower().str.replace(\" \", \"_\")\n",
    "        raw_df = raw_df.drop_duplicates().reset_index(drop=True)\n",
    "\n",
    "        # required\n",
    "        req = [\"case_id\", \"female_id\", \"male_id\"]\n",
    "        missing = [c for c in req if c not in raw_df.columns]\n",
    "        if missing:\n",
    "            raise ValueError(f\"Missing required columns: {missing}\")\n",
    "\n",
    "        # optional standardize episode\n",
    "        if \"et_date\" in raw_df.columns:\n",
    "            raw_df[\"transfer_time_id\"] = pd.to_datetime(raw_df[\"et_date\"], errors=\"coerce\").dt.strftime(\"%Y-%m-%d\")\n",
    "        else:\n",
    "            raw_df[\"transfer_time_id\"] = None\n",
    "\n",
    "        # open star db connection\n",
    "        conn = sqlite3.connect(STAR_DB)\n",
    "\n",
    "        # process doctor first (autoinc)\n",
    "        raw_df = process_doctors(conn, raw_df, refresh)\n",
    "\n",
    "        # process each dimension sequentially (female, male, protocol, outcome, embryo)\n",
    "        for dim_name, dim_def in DIM_DEFS.items():\n",
    "            id_col = dim_def[\"id_col\"]\n",
    "            if id_col not in raw_df.columns:\n",
    "                raw_df[id_col] = None\n",
    "            raw_df = process_dimension(conn, raw_df, dim_name, dim_def, refresh)\n",
    "\n",
    "        # time dim\n",
    "        build_dim_time(raw_df, conn)\n",
    "\n",
    "        # facts\n",
    "        load_fact_tables(raw_df, conn)\n",
    "\n",
    "        conn.close()\n",
    "        logging.info(\"ETL completed successfully.\")\n",
    "        print(\"ETL Done ✔\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        logging.exception(\"run_full_etl FAILED\")\n",
    "        print(f\"ETL FAILED: {e}\")\n",
    "        return False\n",
    "\n",
    "# ----------------------------\n",
    "# RUN\n",
    "# ----------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    # change to True for initial full refresh\n",
    "    run_full_etl(refresh=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "db8ee5c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                               doctor_recommendation\n",
      "0  Increase gonadotropin dose in next cycle for b...\n",
      "1  Proceed with embryo freezing for future transfer.\n",
      "2  Monitor progesterone closely during luteal phase.\n",
      "3  Consider switching to mild stimulation protoco...\n",
      "4            Good prognosis, continue same protocol.\n",
      "5  Optimize sperm selection for ICSI in next atte...\n"
     ]
    }
   ],
   "source": [
    "import sqlite3\n",
    "import pandas as pd\n",
    "\n",
    "RAW_DB = r\"E:\\work\\DEPI\\graduation promax\\data\\raw\\ivf_database_updated.db\"\n",
    "\n",
    "conn = sqlite3.connect(RAW_DB)\n",
    "\n",
    "query = \"\"\"\n",
    "SELECT DISTINCT\n",
    "    doctor_recommendation\n",
    "FROM ivf_patients\n",
    "WHERE doctor_recommendation IS NOT NULL;\n",
    "\"\"\"\n",
    "\n",
    "df = pd.read_sql(query, conn)\n",
    "conn.close()\n",
    "\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bd1c5f7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
